{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac728771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/39/1e/9dc3ccee95d0e16e54e353d3c355bb7cc506d56a2dbb0a07bc739cc48eac/openai-1.52.0-py3-none-any.whl.metadata\n",
      "  Using cached openai-1.52.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting langchain\n",
      "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/53/b9/290c361a4976947ac6fad11af0a4c11db0b5d8357dc3447d28c1ecd9a1a3/langchain-0.3.4-py3-none-any.whl.metadata\n",
      "  Using cached langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain_openai\n",
      "  Obtaining dependency information for langchain_openai from https://files.pythonhosted.org/packages/66/ea/dcc59d9b818a4d7f25d4d6b3018355a0e0243a351b1d4ef8b26ec107ee00/langchain_openai-0.2.3-py3-none-any.whl.metadata\n",
      "  Using cached langchain_openai-0.2.3-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\macosta\\appdata\\roaming\\python\\python311\\site-packages (from openai) (1.9.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/56/95/9377bcb415797e44274b51d46e3249eba641711cf3348050f76ee7b15ffc/httpx-0.27.2-py3-none-any.whl.metadata\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\macosta\\appdata\\roaming\\python\\python311\\site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.10.8)\n",
      "Requirement already satisfied: sniffio in c:\\users\\macosta\\appdata\\roaming\\python\\python311\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\macosta\\appdata\\roaming\\python\\python311\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (3.8.5)\n",
      "Collecting langchain-core<0.4.0,>=0.3.12 (from langchain)\n",
      "  Obtaining dependency information for langchain-core<0.4.0,>=0.3.12 from https://files.pythonhosted.org/packages/ce/4a/a6499d93805c3e6316e641b6934e23c98c011d00b9a2138835d567e976e5/langchain_core-0.3.12-py3-none-any.whl.metadata\n",
      "  Using cached langchain_core-0.3.12-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
      "  Obtaining dependency information for langchain-text-splitters<0.4.0,>=0.3.0 from https://files.pythonhosted.org/packages/da/6a/d1303b722a3fa7a0a8c2f8f5307e42f0bdbded46d99cca436f3db0df5294/langchain_text_splitters-0.3.0-py3-none-any.whl.metadata\n",
      "  Using cached langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Obtaining dependency information for langsmith<0.2.0,>=0.1.17 from https://files.pythonhosted.org/packages/20/cc/e559a369fd3811534563e568ee7077fdc1698d86d99afe28d5167e692787/langsmith-0.1.136-py3-none-any.whl.metadata\n",
      "  Using cached langsmith-0.1.136-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (1.24.3)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Obtaining dependency information for pydantic<3,>=1.9.0 from https://files.pythonhosted.org/packages/df/e4/ba44652d562cbf0bf320e0f3810206149c8a4e99cdbf66da82e97ab53a15/pydantic-2.9.2-py3-none-any.whl.metadata\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\macosta\\appdata\\roaming\\python\\python311\\site-packages (from langchain_openai) (0.8.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\macosta\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\macosta\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\macosta\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\macosta\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\macosta\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\macosta\\appdata\\roaming\\python\\python311\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.9)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\macosta\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\macosta\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (2.1)\n",
      "Using cached openai-1.52.0-py3-none-any.whl (386 kB)\n",
      "Using cached langchain-0.3.4-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_openai-0.2.3-py3-none-any.whl (49 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached langchain_core-0.3.12-py3-none-any.whl (407 kB)\n",
      "Using cached langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
      "Using cached langsmith-0.1.136-py3-none-any.whl (296 kB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Installing collected packages: pydantic, httpx, openai, langsmith, langchain-core, langchain-text-splitters, langchain_openai, langchain\n",
      "Successfully installed httpx-0.27.2 langchain-0.3.4 langchain-core-0.3.12 langchain-text-splitters-0.3.0 langchain_openai-0.2.3 langsmith-0.1.136 openai-1.52.0 pydantic-2.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script httpx.exe is installed in 'C:\\Users\\macosta\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script openai.exe is installed in 'C:\\Users\\macosta\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script langsmith.exe is installed in 'C:\\Users\\macosta\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script langchain-server.exe is installed in 'C:\\Users\\macosta\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-cloud-auth 0.1.3 requires pydantic<2.0, but you have pydantic 2.9.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip install openai langchain langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2750c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import re\n",
    "import openai\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "#from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77bf0fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "openAIModelName = 'gpt-4-turbo'\n",
    "# 3.5\n",
    "llm_mod1 = ChatOpenAI(model_name = openAIModelName,\n",
    "                        temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "819aa408",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_forrajes = \"\"\"Actúa como observador de colectas que encuentra y extrae la información solicitada del texto. Te voy a proporcionar un texto largo extraído de un PDF que es un informe de colecta.\n",
    "Necesito que extraigas TODA la información específica del texto y de todas las tablas donde encuentres altitud, latitud y longitud y la estructures en formato JSON que contenga todas las accesiones \n",
    "relacionadas en esa colección y asignar a cada una la información correspondiente a los siguientes campos:CIAT No., pais, provincia/estado,fuente de germoplasma (si es 1 es colección CIAT, si es 2 es Coleccion conjunta, si es 3 es Donacion. En caso de que no se entienda, tomar esta información de la segunda fila: si es 1 es coleccion CIAT, si es 2 es Donacion, o si no encuentras esta información, tomarla de \"Observaciones\"), nombre del colector o donante, especie, fecha de colecta o Fecha Col./Intr (SOLO SI HAY información), fecha de registro, lugar de recolección (TODA la información que encuentres), altitud, longitud, latitud. \n",
    "Puede haber más información o menos información que la mencionada. El objetivo que no debes dejar pasar es el identificador de la semilla, altitud, longitud y latitud en TODO el documento y tablas. NO extraigas información de accesiones duplicadas, cada documento corresponde a UN SOLO NÚMERO DE accesión, por lo tanto el json debe tener una sola accesion. No extraigas información sobre la topografía o hábitat nativo, tampoco la palabra carretera si no hay algo escrito ahí. Si alguno de los campos solicitados no está disponible en el documento, por favor asigna un \"no especificado\", pero no te saltes algún campo.\n",
    "\n",
    "Este es un ejemplo de como deberia resultar la data de una accesión:\n",
    "\n",
    "    \"CIAT No.\": \"1\",\n",
    "    \"especie\": \"Stylosanthes scabra\",\n",
    "    \"fuente de germoplasma\": \"Donacion\",\n",
    "    \"colectores\": \"CSIRO Townsville, Australia\",\n",
    "    \"Pais\": \"Brasil\",\n",
    "    \"Provincia/Estado\": \"Sao Paulo\",\n",
    "    \"Lugar recoleccion\": \"Pedro Branca, Sao Paulo\",\n",
    "    \"fecha de colecta\": \"01/01/1968\",\n",
    "    \"fecha de registro\": \"28/07/1971\",\n",
    "    \"altitud\": \"No especificada\",\n",
    "    \"longitud\": \"75.34W\",\n",
    "    \"latitud\": \"02.57N\"  \n",
    "    \n",
    "Aquí está el texto real del PDF que necesito que proceses:\"\"\"\n",
    "\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template_forrajes),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "# No. Donante (si la fuente de germoplasma es 3 o si en la segunda fila hay un 2), No. Colector (si la fuente de germoplasma es 1), No. Campo (si la fuente de germoplasma es 2),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835f300b",
   "metadata": {},
   "source": [
    "# Funcion para extraer datos de pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_filenames_by_length(directory_path, pattern):\n",
    "    matching_files = []\n",
    "    for filename in [os.path.join(directory, file) for file in os.listdir(directory)]:\n",
    "        if re.match(pattern, filename) and 'combined_data' not in filename :\n",
    "            matching_files.append(filename)\n",
    "    return matching_files\n",
    "\n",
    "directory = 'Path to the extractions files' #'/home/ec2-user/SageMaker/pdf_extractions/data/3_out_data/forrajes'\n",
    "parquet_file_path = [os.path.join(directory, file) for file in os.listdir(directory)]\n",
    "files = filter_filenames_by_length(directory, r\".*\\.parquet$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafcdf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(file_path):\n",
    "    # Function to extract the numeric part from the file path\n",
    "\n",
    "    match = re.search(r'/(\\d+)\\.parquet$', file_path)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    " \n",
    " \n",
    "# Sort the list of file paths based on the numeric part\n",
    "sorted_file_paths = sorted(files, key=extract_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f3159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_info(parquet_file, openAIModelName, temperature, chat_prompt):\n",
    "\n",
    "    # Function to extract PDF data\n",
    "\n",
    "   \n",
    "    print(f\"Processing the file {parquet_file}\")\n",
    "   \n",
    "    llm_mod1 = ChatOpenAI(model_name = openAIModelName,\n",
    "                        temperature=temperature)\n",
    "    df = pd.read_parquet(parquet_file, engine='pyarrow')\n",
    "    text = '\\n'.join(df['Texto_Extraido'].tolist())\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    docs = [Document(page_content=x) for x in text_splitter.split_text(text)]\n",
    "   \n",
    "    try:\n",
    "        messages = chat_prompt.format_messages(text=docs)\n",
    "        result = llm_mod1.invoke(messages)\n",
    "        output = result.content\n",
    "       \n",
    "        print(output)\n",
    "       \n",
    "        # Extract JSON data\n",
    "        start_index = output.find('{')\n",
    "        end_index = output.rfind('}') + 1\n",
    "        json_string = output[start_index:end_index]\n",
    "        json_data = json.loads(json_string)\n",
    "       \n",
    "        return json_data\n",
    "       \n",
    "    except :\n",
    "        print(\"Failed to decode JSON from the result content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b83d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the function for all the parquet files\n",
    "\n",
    "results = list()\n",
    "for file in sorted_file_paths:\n",
    "    results.append(extract_pdf_info(file, openAIModelName = openAIModelName, temperature = 0.3, chat_prompt = template_forrajes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ca3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_json_data(file,output_path):\n",
    "    \n",
    "    # append all the results in a txt\n",
    "   \n",
    "        json_dict = {}\n",
    "        for json_data in file:\n",
    "                first_key = list(json_data.keys())[0]\n",
    "                first_value = json_data[first_key]\n",
    "                json_dict[first_value] = json_data\n",
    " \n",
    "        # Convert the dictionary to a JSON-formatted string\n",
    "        json_string = json.dumps(json_dict, indent=4)\n",
    " \n",
    "        # Save the JSON string to a text file\n",
    "        with open(output_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(json_string)\n",
    "       \n",
    "       \n",
    "        print(f\"Output appended to {output_path}\")\n",
    "       \n",
    "        return json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbdee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append all the outputs and save them\n",
    "\n",
    "output_folder = 'Path to write the output'\n",
    "\n",
    "dicts = append_json_data(results,  os.path.join(output_folder,'output_extractions.txt'))\n",
    "  \n",
    "with open('data/output_extractions.txt', 'rb') as file:\n",
    "        dictio = json.load(file)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98080e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert txt to df\n",
    "\n",
    "df = pd.DataFrame(dictio)\n",
    "df_reset = df.reset_index()\n",
    "df_reset = df_reset.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5afa4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save final dataframe\n",
    "\n",
    "df_reset.to_csv(os.path.join(output_folder,'output_extractions.csv'), header = False, index = False, encoding ='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765be20d",
   "metadata": {},
   "source": [
    "## Run try one by one\n",
    "#### DONT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a499e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parquet_file_path = '/home/ec2-user/SageMaker/genebanks/data/3_out_data/Ecogeographic survey of the target species-Costa Rica-2004.parquet'\n",
    "#df = pd.read_parquet(parquet_file_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1111329c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nombre_Carpeta</th>\n",
       "      <th>Nombre_Imagen</th>\n",
       "      <th>Texto_Extraido</th>\n",
       "      <th>conteo_caracteres</th>\n",
       "      <th>Numero_Pagina</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1838</td>\n",
       "      <td>page_1.png</td>\n",
       "      <td>1838\\nCALA\\nPrograma de Pastos Tropicales\\nCIA...</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1838</td>\n",
       "      <td>page_2.png</td>\n",
       "      <td>TARJETA DE RECOLECCION /INTRODUCCION\\nCIAT No....</td>\n",
       "      <td>896</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Nombre_Carpeta Nombre_Imagen  \\\n",
       "102            1838    page_1.png   \n",
       "103            1838    page_2.png   \n",
       "\n",
       "                                        Texto_Extraido  conteo_caracteres  \\\n",
       "102  1838\\nCALA\\nPrograma de Pastos Tropicales\\nCIA...               1968   \n",
       "103  TARJETA DE RECOLECCION /INTRODUCCION\\nCIAT No....                896   \n",
       "\n",
       "     Numero_Pagina  \n",
       "102              1  \n",
       "103              2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_forrajes = pd.read_csv(\"C:/Users/macosta/Downloads/combined_data_forrajes.csv\")\n",
    "df_forrajes = df_forrajes[df_forrajes['Nombre_Carpeta'] == 1838]\n",
    "df_forrajes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03047ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1990, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "# Concatenar el texto extraído de todas las páginas\n",
    "text1 = '\\n'.join(df_forrajes['Texto_Extraido'].tolist())\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs1 = [Document(page_content=x) for x in text_splitter.split_text(text1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51538382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"CIAT No.\": \"1838\",\n",
      "    \"especie\": \"Stylosanthes guianensis\",\n",
      "    \"fuente de germoplasma\": \"Colección conjunta\",\n",
      "    \"colectores\": \"R. Schultze-Kraft, C. Ortega, B. Castillo\",\n",
      "    \"Pais\": \"Panamá\",\n",
      "    \"Provincia/Estado\": \"Coclé\",\n",
      "    \"Lugar recoleccion\": \"Penonomé-Toabré, 4 km al Nordeste de La Pintada\",\n",
      "    \"fecha de colecta\": \"24/01/1978\",\n",
      "    \"fecha de registro\": \"31/01/1978\",\n",
      "    \"altitud\": \"100 m.s.n.m.\",\n",
      "    \"longitud\": \"80W25\",\n",
      "    \"latitud\": \"8N38\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "messages1 = chat_prompt.format_messages(text=docs1)\n",
    "result1 = llm_mod1.invoke(messages1)\n",
    "output1 = result1.content\n",
    "print(result1.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
